---
title: Srcnn论文
categories: study
tags : 机器学习
toc: true
---

# 目标
从低分辨率的图片中恢复为高分辨率的图片，极为映照一种低分辨率的图片映射为高分辨率图片的映射
# 传统方法
外部高低分辨率示例学习
多个低分辨率图像学习相似度
# 术语

- pipeline 流水线，一整套解决方案
- baseline 基础模型，基线
- patch 在CNN学习训练过程中，不是一次来处理一整张图片，而是先将图片划分为多个小的块，内核 kernel (或过滤器或特征检测器)每次只查看图像的一个块，这一个小块就称为 patch，然后过滤器移动到图像的另一个patch，以此类推。
- 卷积 是元素对元素的加法和乘法。对于具有一个通道的图像，
- 字典 基本组成元素,它实质上是对于庞大数据集的一种降维表示。第二，正如同字是句子最质朴的特征一样，字典学习总是尝试学习蕴藏在样本背后最质朴的特征（假如样本最质朴的特征就是样本最好的特征）
- 流行空间 :高维空间有冗余，低维空间没冗余。也就是说，流形可以作为一种数据降维的方式,流形能够刻画数据的本质。也就是说。既然学习到了“将数据从高维空间降维到低维空间，还能不损失信息”的映射，那这个映射能够输入原始数据，输出数据更本质的特征(就像压缩一样，用更少的数据能尽可能多地表示原始数
- mlp (多层感知器（Multi-Layer Perception )最典型的MLP包括包括三层：输入层、隐层和输出层，MLP神经网络不同层之间是全连接的
- 神经网络主要有三个基本要素：权重、偏置和激活函数

	- 权重：神经元之间的连接强度由权重表示，权重的大小表示可能性的大小

	- 偏置：偏置的设置是为了正确分类样本，是模型中一个重要的参数，即保证通过输入算出的输出值不能随便激活。

	- 激活函数：起非线性映射的作用，其可将神经元的输出幅度限制在一定范围内，一般限制在（-1~1）或（0~1）之间。最常用的激活函数是Sigmoid函数，其可将（-∞，+∞）的数映射到（0~1）的范围内。
- ground truth 把它理解为真值、真实的有效值或者是标准的答案。 维基百科对Ground Truth在机器学习领域的解释是： 在机器学习中，“ground truth”一词指的是训练集对监督学习技术的分类的准确性。
- basis 基本组成，高级看待低级就是低级组成的
- 滤波器 滤波器是一个矩阵，大小为m*n它是用来检测图像中特定的特征的，不同的滤波器有不同的参数。我们知道图像在计算机中的数字信号其实是MN3大小的矩阵，假设我们只考虑图像的灰度，不考虑RGB，那么图像的大小为MN。某一个滤波器对图像进行滤波时，就是将滤波器分别与图像的同大小区域进行点乘，每次滤波器依次从左往右从上往下滑过该图像所有的区域，让该滤波器对图像的某一个与滤波器尺寸同大小的图像区域（mn）进行点乘，点乘后各个乘积求和得到新的过滤后的图像，这种图像某一部分与滤波器点乘后求和操作就是以后的卷积神经网络中的卷积操作，这样就得到了经过滤波器过滤后的图像。不同的滤波器可以检测图像的不同特征
- 偏差（bias）：偏差衡量了模型的预测值与实际值之间的偏离关系，也就是输出预测结果的期望与样本真实结果的差距
- Feature Map(特征图)是输入图像经过神经网络卷积产生的结果，表征的是神经空间内一种特征；其分辨率大小取决于先前卷积核的步长 。层与层之间会有若干个卷积核（kernel），上一层中的每个feature map跟每个卷积核做卷积，对应产生下一层的一个feature map

- 通道 和特征图是同一个事情。一层可以有多个通道（或者说特征图）。如果输入的是一个RGB图像，那么就会有3个通道。“channel”通常被用来描述“layer”的结构。相似的，“kernel”是被用来描述“filter”的结构
- 均方误差（MSE）损失函数——L^2L 
2
- 损失函数 平方损失函数得到的是目标值与预测值之间差的平方的绝对值
	- MSE的函数曲线光滑、连续，处处可导，便于使用梯度下降算法。随着误差的减小，梯度也在减小，这有利于收敛，即使使用固定的学习速率，也能较快的收敛到最小值。
	缺点：
	- 当真实值y和预测值f(x)的差值大于1时，平方计算会放大误差，而当差值小于1时，平方计算则会缩小误差。MSE对于较大的误差(>1)给予较大的惩罚，较小的误差(<1）给予较小的惩罚。也就是说，对离群点（奇异值）比较敏感，受其影响较大

- PSNR 峰值信噪比，Peak signal-to-noise ratio（PSNR）是测量有损压缩编／解码器的重建质量的重要指标，在图像处理领域很常见，因为在图像压缩处理过程中，常常会引入噪声，这些噪声就会影响图像重建质量，对于图像重建，较高的PSNR指标通常表明重建质量较高，图像失真越小

- 随机梯度下降(Stochastic Gradient Descent, SGD)是梯度下降算法的一个扩展。机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的训练集的计算代价也更大。机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。随着训练集规模增长为数十亿的样本，计算一步梯度也会消耗相当成的时间。随机梯度下降的核心是：梯度是期望。期望可使用小规模的样本近似估计。具体而言，在算法的每一步，我们从训练集中均匀抽出一小批量(minibatch)样本B={x(1), …, x(m’)}。小批量的数目m’通常是一个相对较小的数，从一到几百。重要的是，当训练集大小m增长时，m’通常是固定的。我们可能在拟合几十亿的样本时，每次更新计算只用到几百个样本。梯度下降往往被认为很慢或不可靠。优化算法不一定能保证在合理的时间内达到一个局部最小值，但它通常能及时地找到代价函数一个很小的值，并且是有用的。
- 学习率 学习率过大的时候会导致模型难以收敛，过小的时候会收敛速度过慢，控制 模型的 学习进度 
- padding
	- valid or no padding:valid顾名思义filter只在valid（有效）区域卷积。如果stride=1，output_size = input_size -( kernel_size -1)
	- Same or half padding:这种padding是让输出尺寸等于输入尺寸的padding。当stride = 1，要补（kernel_size - 1）行（列）零
	- Full padding:Full意味着kernel会一个个遍历所有输入图像像素。当stride = 1，要补2*（kernel_size - 1）行（列）零。

- 随机梯度下降算法（Stochastic gradient descent，SGD）



## 卷积神经网络的一整套流程就是

更新卷积核参数（weights），就相当于是一直在更新所提取到的图像特征，以得到可以把图像正确分类的最合适的特征们。（一句话：更新weights以得到可以把图像正确分类的特征
## 优化

图像恢复（例如，[1]）中流行的策略是密集提取patch，然后通过一组预先训练的基（如PCA，DCT，Haar等）表示它们。这等效于通过一组滤镜对图像进行卷积，每个滤镜都是一个基础。在我们的制定中，我们将这些基的优化纳入网络的优化中 
