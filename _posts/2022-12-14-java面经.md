---
layout: post
title: java面经
categories: study
tags : java
toc: true
---

# java基础
## 常用类
> 使用字符串时，new和""推荐使用哪种方式？

先看看 "hello" 和 new String("hello") 的区别：

当Java程序直接使用 "hello" 的字符串直接量时，JVM将会使用常量池来管理这个字符串；

当使用 new String("hello") 时，JVM会先使用常量池来管理 "hello" 直接量，再调用String类的构造器来创建一个新的String对象，新创建的String对象被保存在堆内存中。

显然，采用new的方式会多创建一个对象出来，会占用更多的内存，所以一般建议使用直接量的方式创建字符串。

>说一说你对字符串拼接的理解

拼接字符串有很多种方式，其中最常用的有4种，下面列举了这4种方式各自适合的场景。

+ 运算符：如果拼接的都是字符串直接量，则适合使用 + 运算符实现拼接；

StringBuilder：如果拼接的字符串中包含变量，并不要求线程安全，则适合使用StringBuilder；

StringBuffer：如果拼接的字符串中包含变量，并且要求线程安全，则适合使用StringBuffer；

String类的concat方法：如果只是对两个字符串进行拼接，并且包含变量，则适合使用concat方法；

> 介绍一下类型擦除

在严格的泛型代码里，带泛型声明的类总应该带着类型参数。但为了与老的Java代码保持一致，也允许在使用带泛型声明的类时不指定实际的类型。如果没有为这个泛型类指定实际的类型，此时被称作raw type（原始类型），默认是声明该泛型形参时指定的第一个上限类型。

当把一个具有泛型信息的对象赋给另一个没有泛型信息的变量时，所有在尖括号之间的类型信息都将被扔掉。比如一个 List<String> 类型被转换为List，则该List对集合元素的类型检查变成了泛型参数的上限（即Object）。

上述规则即为泛型擦除，可以通过下面代码进一步理解泛型擦除：

`List<String> list1 = ...; List list2 = list1; //`

> java的四种引用方式
Java对象的四种引用方式分别是强引用、软引用、弱引用、虚引用，具体含义如下：

强引用：这是Java程序中最常见的引用方式，即程序创建一个对象，并把这个对象赋给一个引用变量，程序通过该引用变量来操作实际的对象。当一个对象被一个或一个以上的引用变量所引用时，它处于可达状态，不可能被系统垃圾回收机制回收。

软引用：当一个对象只有软引用时，它有可能被垃圾回收机制回收。对于只有软引用的对象而言，当系统内存空间足够时，它不会被系统回收，程序也可使用该对象。当系统内存空间不足时，系统可能会回收它。软引用通常用于对内存敏感的程序中。

弱引用：弱引用和软引用很像，但弱引用的引用级别更低。对于只有弱引用的对象而言，当系统垃圾回收机制运行时，不管系统内存是否足够，总会回收该对象所占用的内存。当然，并不是说当一个对象只有弱引用时，它就会立即被回收，正如那些失去引用的对象一样，必须等到系统垃圾回收机制运行时才会被回收。

虚引用：虚引用完全类似于没有引用。虚引用对对象本身没有太大影响，对象甚至感觉不到虚引用的存在。如果一个对象只有一个虚引用时，那么它和没有引用的效果大致相同。虚引用主要用于跟踪对象被垃圾回收的状态，虚引用不能单独使用，虚引用必须和引用队列联合使用
## 类，对象

> 全局变量，局部变量的存储
- 实例变量存储在对象所在的堆内存中。
- 类变量存储在方法区中。
- 局域变量存储在栈内存中


### equals与HashCode

> hashcode方法的作用

1.hashcode特性体现主要在它的查找快捷性，在Set和Map这种使用哈希表结构存储数据的集合中。HashCode方法的就大大体现了它的价值，主要用于在这些集合中确定对象在整个哈希表中存储的区域。
2.如果两个对象相同，则着两个对象的equals方法返回的值一定为true，两个对象的HashCode方法返回的值也一定相同。
3.如果两个对象返回的HashCode的值相同，但不能够说明这两个对象的equals方法返回的值就一定为true，只能说明这两个对象在存储在哈希表中的一个桶中

> 如果一个对象equals方法被重写，那么该对象的HashCode方法也应该被重写

在java中equals方法用于判断两个对象是否相等，而HashCode方法在java中主要由于哈希算法中的寻域的功能（也就是寻找数据应该存储的区域的）。在类似于set和map集合的结构中，java为了提高在集合中查询匹配元素的效率问题，引入了哈希算法，通过某种算法及我们的HashCode方法得到对象的hash码，再通过hash码推算出数据应该存储的位置。然后再进行equals操作进行匹配，减少了比较次数，提高了效率。在集合做了优化之后进行判断元素相等的过程是这样的，首先判断两个对象的HashCode方法返回的值是否相等，如果相等然后再判断两个对象的equals方法，如果HashCode方法返回的值不相等，则直接会认为两个对象不相等，不进行equals方法的判断。有这样一个场景有两个Student对象，equals方法认为如果两个对象的学号相同则认为这两个对象相同。可是如果没有重写HashCode方法只重写了equals方法，此刻并不能实现我们的要求，它首先会判断HashCode方法返回的值是否相等，由于我们没有重写HashCode方法，此时返回的值是不同的，因此不会去判断我们重写的equals方法。而如果重写HashCode方法不重写equals方法也是同样的效果，不重写equals方法实际是调用Object方法中的equals方法，判断的是两个对象的堆内地址。而我们重写的HashCode方法认为相等的两个对象在equals方法处并不相等。因此重写equals方法时一定也要重写HashCode方法，重写HashCode方法时也应该重写equals方法

> 为什么equals方法不相等而HashCode方法返回的值却有可能相同呢？

A：HashCode方法实际上是通过一种算法得到一个对象的hash码，这个hash码是用来确定该对象在哈希表中具体的存储区域的。返回的hash码是int类型的所以它的数值范围为[-2147483648-+2147483647]之间的，而超过这个范围，实际会产生溢出，溢出之后的值实际在计算机中存的也是这个范围的。比如最大值2147483647+1之后并不是在计算机中不存储了，它实际在计算机中存储的是-2147483648。在java中对象可以有很多很多通过new关键字来产生。而hash码也是通过特定算法得到的，所以很难或者说几乎没有什么算法在这个范围内在这个情况下不会不产生相同的hash码的。也就是说在上述情况下肯定是会发生哈希碰撞的，因此不同对象可能有相同的HashCode的返回值。也有人说Object方法中的HashCode方法是通过内存地址得来的，是唯一的。可是HashCode方法是共有的，也就意味着它是可以被程序员重写的。因此不同环境下实现HashCode的算法可能不同。因此equals方法返回结果不相等，而HashCode方法返回的值却有可能相同！

### HashMap的线程安全问题
>HashMap是线程不安全的

多个相同key的同时插入，多个线程同时检测到需要扩容，HashMap 在并发执行 put 操作时会引起死循环，导致 CPU 利用率接近100%。因为多线程会导致 HashMap 的 Node 链表形成环形数据结构，一旦形成环形数据结构，Node 的 next 节点永远不为空，就会在获取 Node 时产生死循环

> 如何在线程安全的时候使用

换用线程安全的HasHMap
Hashtable
ConcurrentHashMap
Synchronized Map
使用Collections将HashMap包装成线程安全的Map。
### HashMap的链表和红黑树转化问题
>HashMap 链表和红黑树的转换

红黑树的平均查找长度是log(n)，长度为8，查找长度为log(8)=3，链表的平均查找长度为n/2，当长度为8时，平均查找长度为8/2=4，这才有转换成树的必要；链表长度如果是小于等于6，6/2=3，虽然速度也很快的，但是转化为树结构和生成树的时间并不会太短。

还有选择6和8的原因是：

中间有个差值7可以防止链表和树之间频繁的转换。假设一下，如果设计成链表个数超过8则链表转换成树结构，链表个数小于8则树结构转换成链表，如果一个`HashMap`不停的插入、删除元素，链表个数在8左右徘徊，就会频繁的发生树转链表、链表转树，效率会很低下.

链表转换为红黑树的最终目的，是为了解决在map中元素过多，hash冲突较大，而导致的读写效率降低的问题。在源码的putVal方法中，有关红黑树结构化的分支为：

>HashMap的底层

HashMap 的底层实现是数组+链表+红黑树的形式的，同时它的数组的默认初始容量是 16、***扩容因子***为 0.75，每次采用 2 倍的扩容。

在 JDK1.7 以及前是在头结点插入的，在 JDK1.8 之后是在尾节点插入的
### HashTable与ConcurrentHashMap
>效率低下的HashTable容器

`HashTable`容器使用`synchronized`来保证线程安全，但在线程竞争激烈的情况下`HashTable`的效率非常低下。因为当一个线程访问`HashTable`的同步方法时，其他线程访问`HashTable`的同步方法时，可能会进入阻塞或轮询状态。如线程1使用`put`进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低

>ConcurrentHashMap的锁分段技术

`HashTable`容器在竞争激烈的并发环境下表现出效率低下的原因，是因为所有访问`HashTable`的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是`ConcurrentHashMap`所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。

`ConcurrentHashMap`是由`Segment`数组结构和`HashEntry`数组结构组成。`Segment`是一种可重入锁`ReentrantLock`，在`ConcurrentHashMap`里扮演锁的角色，`HashEntry`则用于存储键值对数据。一个`ConcurrentHashMap`里包含一个`Segment`数组，`Segment`的结构和HashMap类似，是一种数组和链表结构， 一个`Segment`里包含一个`HashEntry`数组，每个`HashEntry`是一个链表结构的元素， 每个Segment守护着一个`HashEntry`数组里的元素,当对`HashEntry`数组的数据进行修改时，必须首先获得它对应的Segment锁。


如何扩容。扩容的时候首先会创建一个两倍于原容量的数组，然后将原数组里的元素进行再hash后插入到新的数组里。为了高效`ConcurrentHashMap`不会对整个容器进行扩容，而只对某个`segment`进行扩容

如果我们要统计整个`ConcurrentHashMap`里元素的大小，就必须统计所有`Segment`里元素的大小后求和。`Segment`里的全局变量count是一个volatile变量，那么在多线程场景下，我们是不是直接把所有Segment的count相加就可以得到整个`ConcurrentHashMap`大小了呢？不是的，虽然相加时可以获取每个`Segment`的count的最新值，但是拿到之后可能累加前使用的count发生了变化，那么统计结果就不准了。所以最安全的做法，是在统计size的时候把所有`Segment`的put，remove和clean方法全部锁住，但是这种做法显然非常低效。

因为在累加count操作过程中，之前累加过的count发生变化的几率非常小，所以ConcurrentHashMap的做法是先尝试2次通过不锁住Segment的方式来统计各个Segment大小，如果统计的过程中，容器的count发生了变化，则再采用加锁的方式来统计所有Segment的大小。

那么`ConcurrentHashMap`是如何判断在统计的时候容器是否发生了变化呢？使用modCount变量，在put , remove和clean方法里操作元素前都会将变量modCount进行加1，那么在统计size前后比较modCount是否发生变化，从而得知容器的大小是否发生变化
JDK 1.8中的实现：

JDK1.8 的实现已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 Synchronized 和 CAS 来操作，整个看起来就像是优化过且线程安全的 HashMap，虽然在 JDK1.8 中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本。

> concurrentHaahMap是怎么实现分段分组的
get操作：

Segment的get操作实现非常简单和高效，先经过一次再散列，然后使用这个散列值通过散列运算定位到 Segment，再通过散列算法定位到元素。get操作的高效之处在于整个get过程都不需要加锁，除非读到空的值才会加锁重读。原因就是将使用的共享变量定义成 volatile 类型。

put操作：

当执行put操作时，会经历两个步骤：

判断是否需要扩容；

定位到添加元素的位置，将其放入 HashEntry 数组中。

插入过程会进行第一次 key 的 hash 来定位 Segment 的位置，如果该 Segment 还没有初始化，即通过 CAS 操作进行赋值，然后进行第二次 hash 操作，找到相应的 HashEntry 的位置，这里会利用继承过来的锁的特性，在将数据插入指定的 HashEntry 位置时（尾插法），会通过继承 ReentrantLock 的 tryLock() 方法尝试去获取锁，如果获取成功就直接插入相应的位置，如果已经有线程获取该Segment的锁，那当前线程会以自旋的方式去继续的调用 tryLock() 方法去获取锁，超过指定次数就挂起，等待唤醒。

> BlockingQueue是怎么实现的？
参考答案

BlockingQueue是一个接口，它的实现类有ArrayBlockingQueue、DelayQueue、 LinkedBlockingQueue、PriorityBlockingQueue、SynchronousQueue等。它们的区别主要体现在存储结构上或对元素操作上的不同，但是对于put与take操作的原理是类似的。下面以ArrayBlockingQueue为例，来说明BlockingQueue的实现原理。

首先看一下ArrayBlockingQueue的构造函数，它初始化了put和take函数中用到的关键成员变量，这两个变量的类型分别是ReentrantLock和Condition。ReentrantLock是AbstractQueuedSynchronizer（AQS）的子类，它的newCondition函数返回的Condition实例，是定义在AQS类内部的ConditionObject类，该类可以直接调用AQS相关的函数。
```java
public ArrayBlockingQueue(int capacity, boolean fair) {      if (capacity <= 0)          throw new IllegalArgumentException();      this.items = new Object[capacity];      lock = new ReentrantLock(fair);      notEmpty = lock.newCondition();      notFull = lock.newCondition();  }
```
put函数会在队列末尾添加元素，如果队列已经满了，无法添加元素的话，就一直阻塞等待到可以加入为止。函数的源码如下所示。我们会发现put函数使用了wait/notify的机制。与一般生产者-消费者的实现方式不同，同步队列使用ReentrantLock和Condition相结合的机制，即先获得锁，再等待，而不是synchronized和wait的机制。

```java
public void put(E e) throws InterruptedException {      checkNotNull(e);      final ReentrantLock lock = this.lock;      lock.lockInterruptibly();      try {          while (count == items.length)              notFull.await();          enqueue(e);      } finally {          lock.unlock();      }  }
```
再来看一下消费者调用的take函数，take函数在队列为空时会被阻塞，一直到阻塞队列加入了新的元素。

## 流
> 介绍一下java的nio
Java的NIO主要由三个核心部分组成：Channel、Buffer、Selector。

基本上，所有的IO在NIO中都从一个Channel开始，数据可以从Channel读到Buffer中，也可以从Buffer写到Channel中。Channel有好几种类型，其中比较常用的有FileChannel、DatagramChannel、SocketChannel、ServerSocketChannel等，这些通道涵盖了UDP和TCP网络IO以及文件IO。

Buffer本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。Java NIO里关键的Buffer实现有CharBuffer、ByteBuffer、ShortBuffer、IntBuffer、LongBuffer、FloatBuffer、DoubleBuffer。这些Buffer覆盖了你能通过IO发送的基本数据类型，即byte、short、int、long、float、double、char。

Buffer对象包含三个重要的属性，分别是capacity、position、limit，其中position和limit的含义取决于Buffer处在读模式还是写模式。但不管Buffer处在什么模式，capacity的含义总是一样的。

capacity：作为一个内存块，Buffer有个固定的最大值，就是capacity。Buffer只能写capacity个数据，一旦Buffer满了，需要将其清空才能继续写数据往里写数据。

position：当写数据到Buffer中时，position表示当前的位置。初始的position值为0。当一个数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity–1。当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0。当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。

limit：在写模式下，Buffer的limit表示最多能往Buffer里写多少数据，此时limit等于capacity。当切换Buffer到读模式时， limit表示你最多能读到多少数据，此时limit会被设置成写模式下的position值
## 动态代理的两种方式
- JDK动态代理：利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。
- CGlib动态代理：利用ASM（开源的Java字节码编辑库，操作字节码）开源包，将代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。

1.JDK代理使用的是反射机制实现aop的动态代理，CGLIB代理使用字节码处理框架asm，通过修改字节码生成子类。所以jdk动态代理的方式创建代理对象效率较高，执行效率较低，cglib创建效率较低，执行效率高；

2.JDK动态代理机制是委托机制，具体说动态实现接口类，在动态生成的实现类里面委托hanlder去调用原始实现类方法，CGLIB则使用的继承机制，具体说被代理类和代理类是继承关系，所以代理类是可以赋值给被代理类的，如果被代理类有接口，那么代理类也可以赋值给接口

JDK代理只能对实现接口的类生成代理；CGlib是针对类实现代理，对指定的类生成一个子类，并覆盖其中的方法，这种通过继承类的实现方式，不能代理final修饰的

## 线程
### 线程池的参数和创建线程的方式
> 线程池的参数

```java
public ThreadPoolExecutor(
    int corePoolSize,
    int maximumPoolSize,
    loog keepAliveTime,
    TimeUnit unit,
    BlockingQueue<Runnable> workQueue,
    ThreadFactory threadFacrory,
    RejectedExecutionHandler handler

)
```
线程池的构造函数有7个参数，分别是`corePoolSize`、`maximumPoolSize`、`keepAliveTime`、`unit`、`workQueue`、`threadFactory`、`handler`

- 一、`corePoolSize` 线程池核心线程大小

线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会被销毁，除非设置了`allowCoreThreadTimeOut`。这里的最小线程数量即是corePoolSize。任务提交到线程池后，首先会检查当前线程数是否达到了`corePoolSize`，如果没有达到的话，则会创建一个新线程来处理这个任务。

- 二、`maximumPoolSize` 线程池最大线程数量

当前线程数达到corePoolSize后，如果继续有任务被提交到线程池，会将任务缓存到工作队列（后面会介绍）中。如果队列也已满，则会去创建一个新线程来出来这个处理。线程池不会无限制的去创建新线程，它会有一个最大线程数量的限制，这个数量即由maximunPoolSize指定。

- 三、`keepAliveTime` 空闲线程存活时间

一个线程如果处于空闲状态，并且当前的线程数量大于`corePoolSize`，那么在指定时间后，这个空闲线程会被销毁，这里的指定时间由keepAliveTime来设定

- 四、`unit` 空闲线程存活时间单位

`keepAliveTime`的计量单位

- 五、`workQueue` 工作队列

新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列：

 - `①ArrayBlockingQueue`

基于数组的有界阻塞队列，按FIFO排序。新任务进来后，会放到该队列的队尾，有界的数组可以防止资源耗尽问题。当线程池中线程数量达到`corePoolSize`后，再有新任务进来，则会将任务放入该队列的队尾，等待被调度。如果队列已经是满的，则创建一个新线程，如果线程数量已经达到`maxPoolSize`，则会执行**拒绝策略**。

 - `②LinkedBlockingQuene`

基于链表的无界阻塞队列（其实最大容量为`Interger.MAX`），按照`FIFO`排序。由于该队列的近似无界性，当线程池中线程数量达到`corePoolSize`后，再有新任务进来，会一直存入该队列，而基本不会去创建新线程直到maxPoolSize（很难达到Interger.MAX这个数），因此使用该工作队列时，参数`maxPoolSize`其实是不起作用的。

 - `③SynchronousQuene`

一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。也就是说新任务进来时，不会缓存，而是直接被调度执行该任务，如果没有可用线程，则创建新线程，如果线程数量达到maxPoolSize，则执行拒绝策略。

 - `④PriorityBlockingQueue`

具有优先级的无界阻塞队列，优先级通过参数Comparator实现。

- 六、`threadFactory` 线程工厂

创建一个新线程时使用的工厂，可以用来设定线程名、是否为`daemon`线程等等

- 七、`handler` 拒绝策略

当工作队列中的任务已到达最大限制，并且线程池中的线程数量也达到最大限制，这时如果有新任务提交进来，该如何处理呢。这里的拒绝策略，就是解决这个问题的，jdk中提供了4中拒绝策略：

 - `①CallerRunsPolicy`

该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。

 - `②AbortPolicy`

该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。

 - `③DiscardPolicy`

该策略下，直接丢弃任务，什么都不做。

 - `④DiscardOldestPolicy`

该策略下，抛弃进入队列最早的那个任务，然后尝试把这次拒绝的任务放入队列
> 创建线程的方式

- 继承`Thread`
  - 重写 `run` 方法。
  - 然后创建这个子类对象，并调用 `start` 方法启动线程
- `Runnable` ，并实现 `run` 方法，也可以创建一个线程。
  - 首先定义一个类实现 `Runnable` 接口，并实现 `run` 方法。
  - 然后创建 `Runnable` 实现类对象，并把它作为 `target` 传入 `Thread` 的构造函数中
  - 最后调用 `start` 方法启动线程。
- 实现 `Callable` 接口，并结合 `Future` 实现
  - 首先定义一个 `Callable` 的实现类，并实现 `call` 方法。call 方法是带返回值的。
  - 然后通过 `FutureTask` 的构造方法，把这个 `Callable` 实现类传进去。
  - 把 `FutureTask` 作为 `Thread` 类的 `target` ，创建 `Thread` 线程对象。
  - 通过 `FutureTask` 的 `get`方法获取线程的执行结果。
- 通过线程池创建线程
  - 此处用 JDK 自带的 `Executors` 来创建线程池对象。
  - 首先，定一个 `Runnable` 的实现类，重写 `run` 方法。
  - 然后创建一个拥有固定线程数的线程池。
  - 最后通过 `ExecutorService` 对象的 `execute` 方法传入线程对象


> 线程之间的通信方式

1、`wait()`、`notify()`、`notifyAll()`

如果线程之间采用`synchronized`来保证线程安全，则可以利用`wait()`、`notify()`、`notifyAll()`来实现线程通信。这三个方法都不是Thread类中所声明的方法，而是`Object`类中声明的方法。原因是每个对象都拥有锁，所以让当前线程等待某个对象的锁，当然应该通过这个对象来操作。并且因为当前线程可能会等待多个线程的锁，如果通过线程来操作，就非常复杂了。另外，这三个方法都是本地方法，并且被`final`修饰，无法被重写。

`wait()`方法可以让当前线程释放对象锁并进入阻塞状态。`notify()`方法用于唤醒一个正在等待相应对象锁的线程，使其进入就绪队列，以便在当前线程释放锁后竞争锁，进而得到CPU的执行。`notifyAll()`用于唤醒所有正在等待相应对象锁的线程，使它们进入就绪队列，以便在当前线程释放锁后竞争锁，进而得到CPU的执行。

每个锁对象都有两个队列，一个是就绪队列，一个是阻塞队列。就绪队列存储了已就绪（将要竞争锁）的线程，阻塞队列存储了被阻塞的线程。当一个阻塞线程被唤醒后，才会进入就绪队列，进而等待CPU的调度。反之，当一个线程被wait后，就会进入阻塞队列，等待被唤醒。

2、`await()`、`signal()`、`signalAll()`

如果线程之间采用`Lock`来保证线程安全，则可以利用`await()`、`signal()`、`signalAll()`来实现线程通信。这三个方法都是`Condition`接口中的方法，该接口是在Java 1.5中出现的，它用来替代传统的wait+notify实现线程间的协作，它的使用依赖于 Lock。相比使用wait+notify，使用Condition的await+signal这种方式能够更加安全和高效地实现线程间协作。

Condition依赖于Lock接口，生成一个`Condition`的基本代码是`lock.newCondition()` 。 必须要注意的是，`Condition` 的 `await()/signal()/signalAll()` 使用都必须在`lock`保护之内，也就是说，必须在lock.lock()和lock.unlock之间才可以使用。事实上，`await()/signal()/signalAll()` 与 `wait()/notify()/notifyAll()`有着天然的对应关系。即：Conditon中的`await()`对应Object的`wait()`，Condition中的`signal()`对应Object的`notify()`，`Condition`中的`signalAll()`对应Object的`notifyAll()`。

3、`BlockingQueue`

Java 5提供了一个`BlockingQueue`接口，虽然`BlockingQueue`也是Queue的子接口，但它的主要用途并不是作为容器，而是作为线程通信的工具。`BlockingQueue`具有一个特征：当生产者线程试图向`BlockingQueue`中放入元素时，如果该队列已满，则该线程被阻塞；当消费者线程试图从`BlockingQueue`中取出元素时，如果该队列已空，则该线程被阻塞。

程序的两个线程通过交替向BlockingQueue中放入元素、取出元素，即可很好地控制线程的通信。线程之间需要通信，最经典的场景就是生产者与消费者模型，而BlockingQueue就是针对该模型提供的解决方案。
## jvm
### 垃圾回收算法
> 垃圾回收算法

GC 把程序不用的内存空间视为「垃圾」，（几乎所有的）GC 要做的就只有两件事：

找到内存空间里的垃圾，使其和活对象分开来。
回收垃圾对象的内存，使得程序可以重复使用这些内存。

- 基于可达性分析的 GC
  - 基本思路就是通过根集合（gc root）作为起始点，从这些节点出发，根据引用关系开始搜索，所经过的路径称为引用链，当一个对象没有被任何引用链访问到时，则证明此对象是不活跃的，可以被回收。使用此类算法的有JVM、.NET、Golang等。
  -  垃圾回收的效率较高，实现起来比较简单
  -  缺点在于 GC 期间，整个应用需要被挂起（`STW`，`Stop-the-world`，下同），后面很多此类算法的提出，都是在解决这个问题（缩小 `STW` 时间）。
- 基于引用计数法的 GC
  - 在堆内存中分配对象时，会为对象分配一段额外的空间，这个空间用于维护一个计数器，如果有一个新的引用指向这个对象，则计数器的值加1；如果指向该对象的引用被置空或指向其它对象，则计数器的值减1。每次有一个新的引用指向这个对象时，计数器加1；反之，如果指向该对象的引用被置空或指向其它对象，则计数器减1；当计数器的值为0时，则自动删除这个对象。使用此类算法的有 `Python`、`Objective-C`、`Perl`等。
  - 引用计算法是是算法简单，实现较难
  - 天然带有增量特性（incremental），GC 可与应用交替运行，不需要暂停应用；同时，在引用计数法中，每个对象始终都知道自己的被引用数，当计数器为0时，对象可以马上回收，而在可达性分析类 GC 中，即使对象变成了垃圾，程序也无法立刻感知，直到 GC 执行前，始终都会有一部分内存空间被垃圾占用
  - 引用计数算法无法解决「**循环引用无法回收**」的问题，即两个对象互相引用，所以各对象的计数器的值都是 1，即使这些对象都成了垃圾（无外部引用），GC 也无法将它们回收
  - 引用计数算法最大的问题在于：计数器值的增减处理非常繁重，譬如对根对象的引用
  - 多个线程之间共享对象时需要对计数器进行原子递增/递减，这本身又带来了一系列新的复杂性和问题，计数器对应用程序的整体运行速度的影响

真正的工业级实现一般是这两类算法的组合

- 串行执行：垃圾回收器执行的时候应用程序挂起，串行执行指的是垃圾回收器有且仅有一个后台线程执行垃圾对象的识别和回收；
- 并行执行：垃圾回收器执行的时候应用程序挂起，但是在暂停期间会有多个线程进行识别和回收，可以减少垃圾回收时间；
- 并发执行：垃圾回收器执行期间，应用程序不用挂起正常运行（当然在某些必要的情况下垃圾回收器还是需要挂起的）

#### 三色标记算法

>三色标记算法

背后的首要原则就是把堆中的对象根据它们的颜色分到不同集合里面，这三种颜色和所包含的意思分别如下所示：

- 白色：还未被垃圾回收器标记的对象
- 灰色：自身已经被标记，但其拥有的成员变量还未被标记
- 黑色：自身已经被标记，且对象本身所有的成员变量也已经被标记'


在 GC 开始阶段，刚开始所有的对象都是白色的，在通过可达性分析时，首先会从根节点开始遍历，将 GC Roots 直接引用到的对象 A、B、C 直接加入灰色集合，然后从灰色集合中取出 A，将 A 的所有引用加入灰色集合，同时把 A 本身加入黑色集合。最后灰色集合为空，意味着可达性分析结束，仍在白色集合的对象即为 GC Roots 不可达，可以进行回收了

在标记对象是否存活的过程中，对象间的引用关系是不能改变的，这对于串行 GC 来说是可行的，因为此时应用程序处于 STW 状态。对于并发 GC 来说，在分析对象引用关系期间，对象间引用关系的建立和销毁是肯定存在的，如果没有其他补偿手段，并发标记期间就可能出现对象多标和漏标的情况

多标不会影响程序的正确性，只会推迟垃圾回收的时机，漏标会影响程序的正确性，需要引入读写屏障来解决漏标的问题


#### 读屏障（Read barrier）和写屏障（Write barrier）

指的是程序在从堆中读取引用或更新堆中引用时，GC 需要执行一些额外操作，其本质是一些同步的指令操作，在进行读/写引用时，会额外执行这些指令。

读/写屏障实现的是「对 *读/写* 引用这个操作的环切」，即该操作前后都在屏障的范畴内，可以将读/写屏障类比于 Spirng 框架里的拦截器。下面所示的代码，当从 foo 的成员变量第一次从堆上被加载时，就会触发读屏障（后续使用该引用不会触发 ），而当 bar 的成员变量(引用类型的)被分配/写入时，会触发写屏障


方法一：

开启写屏障，当新增引用关系后，触发写屏障，发出引用的黑色或者白色对象会被标记成灰色（例子中 A 将被标记为灰色并进入灰色集合），或者将被引用对象标记为灰色。
开启读屏障，当检测到应用即将要访问白色对象时，触发读屏障，GC 会立刻访问该对象并将之标为灰色。这种方法被称为「增量更新（Increment Update）」。

方法二：

开启写屏障。当删除引用关系前，将所有即将被删除的引用关系的旧引用记录下来（C -> E），最后以这些旧引用为根重新扫描一遍，这种方法实际上是「SATB（Snapshot At The Begining） 算法」



JVM 里还有另外一组内存屏障的概念：读屏障（Load Barrier）和写屏障（Store Barrier），这两组指令和上面我们谈及的屏障不同，Load Barrier 和 Store Barrier主要用来保证主缓存数据的一致性以及屏障两侧的指令不被重排序
#### 基础垃圾回收算法
- 标记-清除算法
  - 标记阶段和清除阶段构成。标记阶段是把所有活动对象都做上标记的阶段，有*对象头标记*和*位图标记*（`bitmap marking`）这两种方式，后者可以与写时复制技术（`copy-on-write`）相兼容。清除阶段是把那些没有标记的对象，也就是非活动对象回收的阶段，回收时会把对象作为分块，连接到被称为「空闲链表（`free-lis`）」的链表中去。
  - 清除操作并不总是在标记阶段结束后就全部完成的，一种「延迟清除（`Lazy Sweep`）」的算法可以缩减因清除操作导致的应用 `STW` 时间。延迟清除算法不是一下遍历整个堆（清除所花费的时间与堆大小成正比），它只在分配对象时执行必要的堆遍历，同时其算法复杂度只与活动对象集的大小成正比
- 标记-压缩算法
  - 清除算法的基础上，用「压缩」取代了「清除」这个回收过程， GC 将已标记并处于活动状态的对象移动到了内存区域的起始端，然后清理掉了端边界之外的内存空间-清除算法的基础上，用「压缩」取代了「清除」这个回收过程， GC 将已标记并处于活动状态的对象移动到了内存区域的起始端，然后清理掉了端边界之外的内存空间
- 标记-复制算法
  - 标记-复制算法与标记-压缩算法非常相似，因为它们会对活动对象重新分配（reloacate）空间位置。两个算法区别是：在标记-复制算法中，reloacate 目标是一个不同的内存区域。


#### 垃圾回收算法的改进

三种垃圾回收算法，会针对基础算法中诸如堆碎片化、暂停时间过长、空间利用率不高等不足进行改进

##### 分代算法（Generational GC）

分代算法对基础算法的改进主要体现在该算法减小了 GC 的作用范围。如前所述，标记过程和对象的 reloacate 过程都需要完全停止应用程序进行堆搜索，堆空间越大，进行垃圾回收所需的时间就越长，如果 GC 的堆空间变小，应用暂停时间也会相应地降低

代算法把对象分类成几代，针对不同的代使用不同的 GC 算法

综合来看，代数划分为 2 代或者 3 代是最好的。

在经过新生代 GC 而晋升的对象把老年代空间填满之前，老年代 GC 都不会被执行。因此，老年代 GC 的执行频率要比新生代 GC 低。通过使用分代垃圾回收，可以改善 GC 所花费的时间（吞吐量）

> 分代算法有着哪些问题

1、不同分代在堆空间之间应当如何划分
有的把老年代的最后一个代通过标记-复制算法处理（Lisp Machine），还有的算法会把最后一个代通过标记-压缩算法回收，降低复制算法出现的频繁换页的问题。

2、如何标记代际之间的引用关系
解决引用问题的关键是引入写屏障：如果一个老年代的引用指向了一个新生代的对象，就会触发写屏障

在写入屏障里，首先会判断：

- 发出引用的对象是不是老年代对象；
- 目标引用标对象是不是新生代对象；
- 发出引用的对象是否还没有加入记录集

> 老年代不用标记-复制算法

老年代一般是难以消亡的，而标记清除算法在对象存活率比较高的情况下就需要进行较多的复制操作，效率将会降低，所以在老年代一般不能选择这种算法。

> 新生代为什么分为Eden和Survivor，他们的比例是多少

新生代分为一个Eden和两个Survivor区。大小比例未8：1：1。

新产生的对象归到新生代的Eden区域中，

新生代的gc多采用标记复制算法。分为两个区域，每次将还活着的对象复制到另一块内存中去，完全清除掉另一块内存空间。运行简单且高效。但是比较浪费空间
。实际上90%的新生代对象熬不过第一次gc。因此在发生垃圾回收时，将Eden和survivor中未被清除掉的对象复制到另一块survivor中，既节省了空间又提高了效率。

> 为什么设置两个survivor区域

浪费的空间只占10%，两个survivor区域可以解决内存碎片化。

> G1垃圾收集器

G1 garbage first面向服务端的垃圾收集器。遵循分代收集理论设计的。但是也是基于区域的理念。将内存分为大小相同的region。每个region分别扮演survivor，eden和老生代区域。g1还将一半region区域大小的对象看作是大对象，一个redion大小以上的对象看作是超大对象，会被连续存放在Humongous Redion中 ，g1会将大对象当作是老年代来看待.每次gc都以一阵个region为单位回收，避免在整个java队中进行垃圾回收。g1维护一个优先级队列，每次在规定收集时间内有限回收收益最大的region。

> cms垃圾处理器

Cms concurrent mark sweep 基于标记清除算法的。追求最短的回收停顿时间
- 初始标记
  - 标记gc root能直接关联到的对象引用
- 并发标记
  - 和当前线程并发执行，标记并发执行前该线程的初始标记之后能关联到的对象
- 重新标记
  - 标记在并发执行中，线程继续运作导致标记记录有所变化的那一部分对象的标记记录，执行时间比初始标记长的那笔并发标记短。
- 并发清除
  - 和当前线程一起并发执行删除清理掉当前已经死亡的对象。

并发执行降低原线程的执行速度和吞吐量。

会产生大量的内存碎片，导致stop the world的full gc的运行


##### 增量算法(Incremental GC）
改进主要体现在该算法通过并发的方式，降低了 STW 的时间。

增量算法的核心思想是：通过 GC 和应用程序交替执行的方式，来控制应用程序的最大暂停时间。

增量算法的「增量」部分，主要有「增量更新（Incremental Update）」和「增量拷贝（Incremental Copying）」两种，前者主要是做「标记」增量，后者是在做「复制」增量。

漏标问题通过写屏障来实现

增量算法中大量使用了读写屏障（主要是写屏障），给应用程序带来了负担，结果就是 GC 的吞吐相较于其他的算法来说不高。
##### 并发算法（Concurrent GC）
广义上的并发算法指的是在 GC 过程中存在并发阶段的算法，如 G1 中存在并发标记阶段，可将其整个算法视为并发算法。

狭义上的并发垃圾回收算法是以基础的标记-复制算法为基础，在各个阶段增加了并发操作实现的。与复制算法的3个阶段相对应，分为并发标记（mark）、并发转移（relocate）和并发重定位（remap）

1）并发标记

从 GC Roots 出发，使用遍历算法对对象的成员变量进行标记。同样的，并发标记也需要解决标记过程中引用关系变化导致的漏标记问题，这一点通过写屏障实现；

（2）并发转移

根据并发标记后的结果生成转移集合，把活跃对象转移（复制）到新的内存上，原来的内存空间可以回收，转移过程中会涉及到应用线程访问待转移对象的情况，一般的解决思路是加上读屏障，在完成转移任务后，再访问对象；

（3）并发重定位

对象转移后其内存地址发生了变化，所有指向对象老地址的指针都要修正到新的地址上，这一步一般通过读屏障来实现。


## jvm 运行java的阶段
> 类加载的过程

- 加载
  - 通过全类名获取类的二进制字节流
  - 将类的静态存储结构加载为方法区的可运行数据结构
  - 内存中生成类的一个Class对象作为方法区该类的信息和结构的入口
- 连接
  - 验证
    - 文件格式验证，元数据验证，字节码验证和符号引用验证
    - 校验数据是否符合jvm的规范，是否有未加载的引用等
  - 准备
    - 将类的静态数据等分配内存并初始化零值
  - 解析
    - jvm将常量池中的符号引用替换为直接引用
- 初始化
  - 为java类中的变量赋予clinit方法即程序员规定的初始化值
- 使用
- 卸载

> 对象实例化的过程

使用字节码中的`init`方法进行初始化

- 先静态，后非静态
- 先父类,后子类 
- 先变量，后代码块
- 先代码块，后构造方法

 
父类静态变量->父类静态代码块->子类静态变量->子类静态代码块->父类变量->父类代码块->父类构造方法->子类变量->子类代码块->子类构造方法

>元空间

在栈外，元空间占用的是本地内存。

> jvm的类加载模型 以及 双亲委派模式

区别不同的类的前提是同一个类加载器加载的类，不同加载器加载的类注定是不同的。

类加载器有 启动类加载器->扩展类加载器->应用程序类加载器->自定义类加载器


启动类加载器加载`JAVA_HOME/lib`目录下的类

扩展类加载器加载的是`\lib\ext`目录之下的。这是一种类似于java库的扩展装置。

应用程序类加载器默认加载的是用户路径下的加载器，别名系统类加载器

双亲委派机制
记载器虽是继承关系但实际上的使用是组合关系。

有个加载器开始加载程序时，需要先判断是否加载过，然后未加载过的类使用父类的加载器进行加载，保证如Object等类在各个类加载环境下都是一致的。没有自定义加载器的化默认使用应用程序类加载器。


## 内存
> 内存溢出和内存泄漏

- 内存溢出：需要的内存超出系统所能够提供的内存且无法再次申请到足够的内存空间
  - 原因
    - 内存中加载的数据来过于庞大
    - 集合中有对对象的引用，使用完后没有清空
    - 代码中存在死循环，产生大量的重复的对象实体
    - 运行之前jvm申请的内存空间太小
    - 第三方软件的bug
  - 建议
    - 程序运行前jvm申请较大的内存空间，修改启动参数申请更大内存
    - 查看错误日志，查看oomError错误之前是否还有其他的异常或者错误
    - 对代码进行排除和分析，找出内存溢出的位置并解决
    - 使用内存查看工具动态查看内存的使用情况。
- 内存泄漏： 程序运行过程中产生的临时变量没有被GC回收，始终占用着内存，内存既不能够被使用也不能够分配给其他程序就发生了内存泄露
  - 内存泄漏的主要原因是长生命周期的对象持有短周期对象的引用
  - 类型
    - 常发性内存泄漏--多次执行内存泄漏的代码
    - 偶发式内存泄漏--特定环境和特定操作下才会产生
    - 一次性内存泄漏--只发生一次
    - 隐式内存泄漏--不及时释放内存，到最后才释放内存可能会耗尽机器的所有内存
  - 建议
    - 今早释放无用对象的引用
    - 少使用静态类型
    - 字符串的处理少使用String多使用StringBuilder或者StringBuffer
    - 避免在循环中创建对象

> 那些区域会发生oomError

除了程序计数器区域外的其他jvm内存区域都有产生oom异常的可能

- java堆溢出
- 方法区溢出
  - 在运行生成大量动态类的时候就会发生内存泄漏。程序可以使用GGlib字节码增强或者动态语言，或者jsp（jsp会编译成为java的一个类）
- 虚拟运行栈 和 本地方法栈
  - 允许动态扩展，当申请不到足够的内存的时候就会发生内存溢出
- 本地直接内存溢出

# 框架

## Spring
> spring 和SpringBoot的区别

spring是一系列java开发的基础框架。包含很多基础的部分比如spring aop,spring orm,spring test,spring mvc,spring security等。简化了java项目的开发。

springboot是spring框架的扩展。消除了设置spring框架的xml的配置，可以更快更高效的开发程序。

spring boot能独立开发spring框架应用。
能够通过starter快速高效的配置
嵌入式tomcat，jetty容器
尽可能的自动配置spring应用


> 简述一下自动装配

自动装配就是通过注解或者一些简单的配置在springboot的帮助下实现某下功能。

`@SpringBootApplication`注解包含有三个注解

- `@Configuration` 允许在上下文中注册额外的bean或者其他的配置类
- `@EnableAutoConfiguration` 启用springboot的自动配置注解
- `@ComponentScan` 规定springboot可以扫描的包，也可以不扫描的包

1、首先是判断`spring.boot.enableautoconfiguation=true` spring boot的自动装配是否开启
2、根据`@ComponentScan`扫描出所有引入的依赖之下的`META_INF/spring.factories`下的所有配置类。
3、根据注解`@ConditionOnXXX`注解判断是否满足条件。将满足条件的bean注入到容器中。


>拦截器和过滤器的区别

过滤器`filter`

拦截器 `interceptor`

过滤器（`Filter`）
过滤器，是在`java web`中将你传入的`request`、`response`提前过滤掉一些信息，或者提前设置一些参数。然后再传入Servlet或Struts2的 action进行业务逻辑处理。比如过滤掉非法url（不是login.do的地址请求，如果用户没有登陆都过滤掉），或者在传入Servlet或Struts2的action前统一设置字符集，或者去除掉一些非法字符。

拦截器（`Interceptor`）
拦截器，是面向切面编程（`AOP`，Aspect Oriented Program）的。就是在你的Service或者一个方法前调用一个方法，或者在方法后调用一个方法。比如动态代理就是拦截器的简单实现，在你调用方法前打印出字符串（或者做其它业务逻辑的操作），也可以在你调用方法后打印出字符串，甚至在你抛出异常的时候做业务逻辑的操作。

通俗理解：
- （1）过滤器（`Filter`）：当你有一堆东西的时候，你只希
- 望选择符合你要求的某一些东西。定义这些要求的工具，就是过滤器。（理解：就是一堆字母中取一个B）
- （2）拦截器（`Interceptor`）：在一个流程正在进行的时候，你希望干预它的进展，甚至终止它进行，这是拦截器做的事情。（理解：就是一堆字母中，干预它，通过验证的少点，顺便干点别的东西）

二、拦截器与过滤器的区别
区别：
- ①：拦截器是基于java的反射机制的，而过滤器是基于函数的回调。
- ②：拦截器不依赖于servlet容器，而过滤器依赖于servlet容器。
- ③：拦截器只对action请求起作用，而过滤器则可以对几乎所有的请求起作用。
- ④：拦截器可以访问action上下文、值、栈里面的对象，而过滤器不可以。
- ⑤：在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次。
<!-- - ⑥：拦截器可以获取IOC容器中的各个bean，而过滤器不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。 -->

三、拦截器与过滤器的触发时机

拦截器与过滤器触发时机不一样

过滤器是在请求进入容器后，但请求进入servlet之前进行预处理的。请求结束返回也是，是在servlet处理完后，返回给前端之前。

过滤器包裹servlet，servlet包裹住拦截器

四、使用场景

SpringMVC的处理器拦截器类似于Servlet开发中的过滤器Filter，用于对处理器进行预处理和后处理。

- 1、日志记录：记录请求信息的日志，以便进行信息监控、信息统计、计算PV（Page View）等。
- 2、能权限检查：如登录检测，进入处理器检测检测是否登录，如果没有直接返回到登录页面；
- 3、性监控：有时候系统在某段时间莫名其妙的慢，可以通过拦截器在进入处理器之前记录开始时间，在处理完后记录结束时间，从而得到该请求的处理时间（如果有反向代理，如apache可以自动记录）；
- 4、通用行为：读取cookie得到用户信息并将用户对象放入请求，从而方便后续流程使用，还有如提取Locale、Theme信息等，只要是多个处理器都需要的即可使用拦截器实现。
- 5、OpenSessionInView：如hibernate，在进入处理器打开Session，在完成后关闭Session。


> 多级缓存架构设计

三级缓存思路 nginx，redis，jvm本地缓存

时效性高的数据信息，采取数据库和redis双写的方式

时效性低的数据，采取MQ异步通知的方式，数据生产服务，异步拉取服务的数据，更新tomcat，jvm，redis的数据

nginx->redis->tomcat+jvm->数据库

nginx 针对高热点的高并发访问

redis针对较高热点，大规模的离散访问

jvm堆内缓存保证缓存雪崩时减少数据库直接承受的压力.

# 数据库
## acis
原子性、一致性、隔离性、持久性。
## 事务
> 事务的分类

- 扁平事务
  - 最简单的事务，从BEGIN WORK开始到COMMIT或者ROLLBACK WORK结束，要么全执行，要么全不执行
- 带有保存点的事务
  - 一个事务如果执行出现异常，全部放弃有点可惜，子啊事务中添加保存点，可以将数据库回滚到保存点发生的时间。
- 嵌套事务
  - 有一个顶级事务控制整体的流程，1有多个局部事务控制局部的变换。顶层事务之下的事务称为子事务。
- 链式事务
  - 保存点事务的一个变种。在提交事务的时候释放不需要的对象，将必要的上下文隐式的传递给下一个要开始的事务，提交事务和下一个事务开始是一个原子操作。下一个事务能够看得到上一个事务的结果就好像是同一个事务中一样。
- 分布式事务


## 关系型数据库

> 数据库常见问题和解决方案


- 缓存穿透
  - 缓存中没有值，数据库中也没有值但是一直有大量的该数据的请求，每次先查缓存再差数据库浪费资源
  - 在第一次查询没有结果之后，在缓存中加入到这个key，设置值为null
  - 对请求参数做校验，过滤掉不，拦截不合适的请求
  - 使用布隆过滤器记录下来未存在的值，已经查过了就不再去查
- 缓存击穿
  - 某时间大量的请求来查询某个值导致数据库崩溃
  - 缓存中key的过期时间保证在热点期中保留较长的时间
  - 使用互斥锁，在第一个线程访问查找的适合锁住，查完值将值存储到缓存当中之后，再释放锁。
  - 定时更新缓存
- 缓存雪崩 
  - 多个key突然在缓存中失效
  - 每个key的过期时间不一致
  - 使用高性能的分布式服务器
- 双写不一致
  - 缓存中的值和数据库的值不一致
  - 先删除缓存再删除数据库中的值
  - 设置key的过期时间比较低，某些场景之下可以
  - 维护一个队列，先更新数据库再删除缓存，删除失败就放进队列，另一个任务不断的从队列中取任务尝试删除key
  - 维护一个读写队列，读写串行化，对于写操作，删除缓存后，写操作放进队列中等待




>mvcc  多版本并发控制

在读已提交和可重复读的隔离级别下实现

`InnoDB` 的 MVCC ，是通过在每行记录后面保存**两个隐藏的列**来实现的。这两个列一个保存了行的创建时间，一个保存行的过期时间(或删除时间)，当然存储的并不是真正的时间，而是系统版本号。每开始一个事务，系统版本号就会自动递增，事务开始时刻的版本号作为当前事务的版本号，用来和查询到的每行记录的版本号就行比较。

以下是 `REPEATABLE READ` 的隔离级别下具体操作：

SELECT
InnoDB 会根据以下两个条件检查每行记录：
a. InnoDB 只查询版本早于当前事务版本的数据行（也就是，行的系统版本号小于或等于事务的系统版号），这样可以确保事务读取的行，要么是在事务开始前的已经存在的，要么是事务自身插入或者修改过的。
b. 行的删除版本要么未定义，要么大于当前事务版本号。这可以确保事务读取到的行，在事务开始之前未被删除。
只有符合上述两个条件的记录，才能返回作为查询结果
- INSERT
  - InnoDB 为新插入的每一行保存当前系统版本号作为行版本号
- DELETE
  - InnoDB 为删除的每一行保存当前系统版本号作为行删除标识
- UPDATE
  - InnoDB 为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识
  
保存着两个额外的系统版本号，使大多数读操作都可以不用加锁。这样设计使得读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行

mvcc的实现技术
- 隐藏列，一个是本行数据的事务id。指向undo log的指针等
- 基于undo log的版本链。每个undolog的日志中都会有指向上一个版本的undolog的指针，从而形成一条版本链
- ReadView 通过隐藏列和版本链可以将数据恢复到指定版本。版本的指定是由readview来指定的。事务在某一时刻给系统打快照，，之后在进行读操作的时候，会将读取到的数据id与快照对比，从而判断数据是是否对readview可见即是否对事务可见

>mysql的acid特性

- 原子性
  - undo log 是一种回滚日志，undolog记录事务开始前老版本数据，用于实现回滚，保证原子性，实现MVCC，会将数据修改前的旧版本保存在undolog，然后行记录有个隐藏字段回滚指针指向老版本。是一种逻辑日志，做之前相反的操作回滚到修改之前。
- 持久性
  - redo log是一种物理日志，会记录事务开启后对数据做的修改，crash-safe。特性：空间一定，写完后会循环写，有两个指针write pos指向当前记录位置，checkpoint指向将擦除的位置，redolog相当于是个取货小车，货物太多时来不及一件一件入库太慢了这样，就先将货物放入小车，等到货物不多或则小车满了或则店里空闲时再将小车货物送到库房。用于crash-safe，数据库异常断电等情况可用redo log恢复
- 隔离性
  - 锁+mvcc
- 一致性
  - 以上三种特性就是为了保证一致性

>mysql的主从复制

1. 主服务器将数据更改记录到二进制文件binlog中
2. 从服务器把主服务器的二级制日志复制到自己的relay日志中
3. 从服务器重做relay日志，将更改应用到自己的数据库中。
  
不是完全实时而是异步实时

### 索引
>mysql的索引

索引是一种在磁盘上存储的特殊物理结构，其中存储着表的每一行数据的引用。

- 普通索引和唯一索引
  - 普通索引可为空，可重复
  - 唯一索引不可重复，可为空
  - 主键索引不可重复，不为空
- 单列索引和组合索引
  - 组合索引遵循最左原则，sql语句中使用到了组合索引中的最左边的索引，这个sql语句就可以使用这个组合索引进行匹配
- 全文索引
  - 对于char，varchar，text，fulltext的列中使用索引，在列上支持全问文找
- 空间索引
  - 对空间类型且not null的字段进行的索引，只能在myisam引擎的表中存储。

可以到单独创建index也可以在创建表的时候创建index。在创建表的时候添加约束，主键约束，唯一约束，外键约束，定义约束的同时就相当于在列上创建了唯一的索引。

> 判断sql有没有走索引，index有没有生效

用`explain`语句

`explain select * from book where book_id = 1 ;`

> 创建索引的原则

- 数据量小的表不需要创建index
- 更新过多的表不需要创建index
- 唯一性的列可以创建唯一索引
- 经常orderby的列或者groupby的几个列可以进行index
- 数据比较单一，例如性别只有男女就不需要建立索引 
- 频繁更新的列，参与列计算的列，不参与列计算的列不适合建索引

> 数据库索引失效
- 组合索引时，遵循最左原则
- 不在索引列上做任何操作，计算函数类型转换都会使得索引失效
- 尽量使用覆盖索引（index 列）少用`select *` 减少回表次数
- mysql在索引列上使用`!=`或者`<>`的时候索引会失效
- 使用 link '%' %开头的时候，索引会失效，转为全表扫描
- 少用 or or 连接的时候会发生全表扫描 
- innodb中发生了隐式转换,字符串不加单引号导致隐式转换
  - 在两边数据类型不一致时，数字类型默认会转化为精度更高的比较，字符串和整数默认转化为浮点数进行比较，当对索引列上的值进行操作的时候索引就会失效

> 索引的实现原理

索引的底层实现都是和表的存储引擎相关的。

MyISAM的底层实现。索引是以B+树的形式存储。主索引和辅助索引没有什么区别，只是主索引不允许重复，主索引的data域存储的是行记录数据地址的引用。辅助索引的data域记录数据记录的地址。

Innodb也是以B+树的形式存储的。只是Innodb本身表数据就是一个B+树。innodb表会按照指定的主键建立b+树，若是没有指定会选择一个不重复的列作为主键，或者自己生成一个6字节的长整型数字作为主键存储数据地址。 innodb的辅助索引的data域会存放数据的主键。查询辅助索引会再次查询主索引。相当于查询两次索引找到数据。

过长的字段，或者非单调的字段不适合作为innodb的主键。而推荐使用自增主键。

> 数据库索引的重建过程

> 数据库索引什么时候重建

表上频繁发生delete，insert操作的时候
发生了after table move move导致rowid发生变化

>怎么判断表索引需要重建了

看表索引是否倾斜的严重，是否浪费空间，对表索引进行结构分析

`analyze index index_name validate structure;`

在相同的session中查询index_stats表

`select height,DEL_LF_ROWS/LF_ROWS from index_stats;`

`height>=4`或者`ELD_LF_ROWS/LF_ROWS>0.2`的情况下就应该考虑重新建立索引

> 如何重建索引

1、drop掉原索引然后再创建索引
```sql
drop index index_name;
create index index_name on table_name (column_name);
```
2、直接重建索引

```sql
alter index index_name rebuild;
alter index index_name rebuild online;
```
此方法较快。使用原有索引项重建索引。

若是在重建索引的过程当中有其他用户在对这个表进行操作，尽量使用online来避免索引重建的时候表的加锁问题。但是本方法需要额外的物理空间。索引建立完成后将老索引删除，若是重建失败也不会影响原来的索引，可以使用本方法来进行索引物理空间的迁移。


rebuild重建索引的过程。

- 1、rebuild采用index fast scan 或者table full scan的方式读取原索引中的数据重建一个索引，采用哪种取决于cost。rebuild online采用全表扫描的方法获取数据。rebuild过程当中有排序操作。
- 2、rebuild会阻塞DML操作，rebuild online不会阻塞DML操作
- 3、rebuild online过程当中系统会产生一个系统临时日志表，临时日志中记录online过程当中索引的变化，系统将日志表中的记录维护到新的索引之后就删除老索引。

注意事项

- 1、注意新建索引有充足的物理空间
- 2、虽然online不阻塞DML操作但还是尽量选在DML操作最少的时间段内进行。
- 3、rebuild操作会产生大量的redo log

> 如何在in 模糊查询使用索引

in字符串要添加单引号

模糊查询`"%"`百分号放在开头是不能够使用索引`index`。可以加入冗余列，冗余列为模糊查询列的倒序列、反转序列。

> index为什么使用B+树

因为B+树的查询次数在2~4次之间，高扇出性

> hash查询和B+树查询的区别

hash索引的底层就是Hash表，往往只通过hash计算就可以得到对应的键值，然后回表查询数据，而b+树的查询是从根节点出发知道查找到叶子节点才算结束。一般要经历2~4次查询，再根据具体情况决定要不要回表查询。

- hash的性能不稳定，数据量过大容易发生hash碰撞而B+树性能稳定。
- hash不支持索引进行排序，b+树支持
- hash不支持范围查询，模糊查询，最左索引匹配，而b+树支持
- 有些时候b+树不需要回表查询例如聚簇索引，Innodb的主键索引

> 特殊情况下使用b+树查询
> 聚簇索引和非聚簇索引

聚簇索引是将数据的索引和数据行对应的放在一起，找到索引就找到了数据。

非聚簇索引是将索引放在内存中，通过查询内存中的索引再去磁盘中查询真正的数据行的存储位置

聚簇索引之上建立的都是辅助索引，非聚簇索引都是辅助索引。

一个表只有一个聚簇索引，设置主键做为聚簇索引的时候最好将主键设置为`auto_increment`自增。

聚簇索引的优势

- 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，再次访问的时候，会在内存中完成访问，不必访问磁盘。这样主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回了，如果按照主键Id来组织数据，获得数据更快。
- 辅助索引使用主键作为"指针"而不是使用地址值作为指针的好处是，减少了当出现行移动或者数据页分裂时辅助索引的维护工作，使用主键值当作指针会让辅助索引占用更多的空间，换来的好处是InnoDB在移动行时无须更新辅助索引中的这个"指针"。也就是说行的位置（实现中通过16K的Page来定位）会随着数据库里数据的修改而发生变化（前面的B+树节点分裂以及Page的分裂），使用聚簇索引就可以保证不管这个主键B+树的节点如何变化，辅助索引树都不受影响。
- 聚簇索引适合用在排序的场合，非聚簇索引不适合
- 取出一定范围数据的时候，使用用聚簇索引
- 二级索引需要两次索引查找，而不是一次才能取到数据，因为存储引擎第一次需要通过二级索引找到索引的叶子节点，从而找到数据的主键，然后在聚簇索引中用主键再次查找索引，再找到数据
- 可以把相关数据保存在一起。例如实现电子邮箱时，可以根据用户 ID 来聚集数据，这样只需要从磁盘读取少数的数据页就能获取某个用户的全部邮件。如果没有使用聚簇索引，则每封邮件都可能导致一次磁盘 I/O。

聚簇索引的劣势

- 维护索引很昂贵，特别是插入新行或者主键被更新导至要分页(page split)的时候。建议在大量插入新行后，选在负载较低的时间段，通过OPTIMIZE TABLE优化表，因为必须被移动的行数据可能造成碎片。使用独享表空间可以弱化碎片
表因为使用UUId（随机ID）作为主键，使数据存储稀疏，这就会出现聚簇索引有可能有比全表扫面更慢，

- 主键的值是顺序的，所以 InnoDB 把每一条记录都存储在上一条记录的后面。当达到页的最大填充因子时（InnoDB 默认的最大填充因子是页大小的 15/16，留出部分空间用于以后修改），下一条记录就会写入新的页中。一旦数据按照这种顺序的方式加载，主键页就会近似于被顺序的记录填满（二级索引页可能是不一样的）

如果主键比较大的话，那辅助索引将会变的更大，因为辅助索引的叶子存储的是主键值；过长的主键值，会导致非叶子节点占用占用更多的物理空间

> B+树的优势

优势提醒在查找效率上。

B+树的树干上只有索引，数据存在叶子节点上，因此更加矮胖，IO次数更少。

B+树每次必须查找到叶子节点上，查找效率稳定。

1.单一节点存储更多的元素，使得查询的IO次数更少；
2.所有查询都要查找到叶子节点，查询性能稳定；
3.所有叶子节点形成有序链表，便于范围查询。

### sql

>分页查询的偏移量非常大的时候如何优化查询

- 在页面中限制分页的数量
- 优化大偏移量的性能
  - 先按照limit 10000，20 查询对应的id主键，再按照id或主键查询需要的行列，做一次关联操作
  - 使用一些唯一的或者有大小顺序的列作为书签，记录下来low-high，再找个基础进行查询

> 预防sql注入

- 严格的参数校验
- sql预编译
### 范式
一般满足第三范式即可
- 第一范式
  - 每个属性被严格分割，没有出现数组、集合之类的列，每一列都是不可分割的原子列
- 第二范式
  - 每个非码属性都必须依赖侯选码，每个示例或者记录都必须严格的区分
- 第三范式
  - 减少数据冗余，，减少传递依赖。一个关系中不包含在其他关系中已经包含的非主关键字信息
## 锁
锁的类型
- 共享锁S lock 允许事务读一行数据
- 排他锁X lock 允许数据删除或更新一行数据

s，x都是行锁

锁的粒度
行级的锁和表级别的锁，额外的锁，意向锁
意向锁是将锁定的对象分为多个层次，意向锁意味着事务希望在更细粒度上进行加锁。

- 意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁。
- 意向排他锁（IX Lock），事务想要获得一张表中某几行的排他锁。

于InnoDB存储引擎支持的是行级别的锁，因此意向锁其实不会阻塞除全表扫以外的任何请求。

锁的算法
= record lock 单个记录上的锁
- gap lock 间隙锁，锁定一个范围但不包含锁本身
- Next-key lock ：gap lock+record lock锁定一个范围同时锁定记录本身
> 间隙锁

间隙锁属于行级锁，锁定一个范围，但是不包含记录本身，避免多个事务将记录插入到同一个范围中，避免发生幻读的情况
>innodb的行级锁是怎么实现的

给索引上的索引项加锁来实现的，通过索引条件来查询才会使用行级锁，否则使用行锁

> 锁升级

一般指行锁升级到页锁，页锁升级到表锁，innodb一般是以页锁为基本单位无论加锁的是一行还是多行数据不存在升级问题，开销基本一致
### 数据库的优化
> 数据库的优化

减少系统的瓶颈，减少资源的占用，增加系统的相应速度，

- 优化文件系统，提示io速度。
- 优化系统操作调度策略，提高mysqk在高负荷情况下的负载能力
- 优化表结构，索引查询语句等是查询结果更快
  
- 针对查询，使用索引，使用连接的方式代替子查询
- 针对慢查询，通过查询慢查询日志，分析原因有针对的优化
- 针对插入，可以先禁用索引和检查，在插入完成之后再开启索引和检查
- 针对表结构，可以擦划分较大的表分为多张表，增加中间表，增加冗余字段等方式进行优化。

> 数据库的日志

- binlog
  - 以二进制的形式存储的日志。记录了所有的对于数据库的操作，包括执行时间，执行消耗的资源，以及相关的事务信息，是默认开启的
- redo log
  - 重做日志是为了实现日志的持久性，一般分为内存中的重做日志缓存，是易失的，和重做日志文件，是持久的。Innodb 是事务的存储引擎，innodb force at commit，当事务提交的时候，必须先将该事务的所有日志写进重做日志文件进行持久化等待该事务的commit操作完成才算完成一个事务。这里的日志指的是redo log 和undolog undolog保证的是数据的原子性。用于事务的回滚和mvcc功能。redo log基本上是顺序写的，数据库允许是不需要对redo log进行读操作。undo log需要进行随机读写的。
- undolog
  - undo存放在数据库内部的一个特殊空间内，成为undo段，位于表共享空间内

### 数据库的引擎
- Innodb引擎
  - Innodb引擎是一个具有提交回滚崩溃恢复处理能力的acid存储引擎，Innodb在锁定行之后也为select语句提供一个非锁定读。Innodb的表可以与其他类型的表一起存储，甚至是在同一个查询中混合。
  - Innodb引擎是为巨大数据量处理提供的最大性能设计，cpu效率高于其他类型的引擎
  - Innodb存储引擎完全与mysql服务器整合。为内存中缓存数据和索引而维护自己的缓冲池。Innodb将它的表和索引存储在一片逻辑空间中，表空间可以包含数个文件，可以是任意尺寸，即使超出文件大小有限定的操作系统上。
  - Innodb支持外键完整性约束。Innodb为每行数据提供一个主键，若是没有显示定位的话，就生成一个6B的rowid来表示主键。
  - Innodb被应用到需要高性能的大型数据库站点上。Innodb自己不创建目录，mysql将在数据目录下创建一个字段扩展文件，和两个日志文件
- MyISAM，扩展ISAM引擎，较高的查询和插入速度
  -  在支持大文件的操作系统上被支持
  -  当把删除和更改插入操作混合使用的时候，动态尺寸的行会产生更多的碎片需要合并下一块空间进行扩展
  -  每个MyISAM最大的索引数是64，可以通过重新编译来改变，每个索引的列可以有16个
  -  最大的键长度为1000B，长度超过250B后，一个超过1024的键将被用上。
  -  BLOB和text的列可以被索引
  -  null允许出现在索引的列里面
  -  数字以高位数字优先被存储，以允许一个更高的索引压缩
  -  每个表的auto_increment的内部处理，，插入删除的时候auto_increment更快。但是序列顶的值被删除之后就不能够在被使用
  -  可以把数据文件和索引文件放在不同的空间存储
  -  varchar列和char列可以固定或者动态记录长度
  -  不同的列可以有不同的字符集
  -  varchar列和char列可以最多64kb

## NoSql数据库

> redis速度快的原因

- redis将数据存储在内存中相比于数据库中的读取要快
- redis是单线程的省去很多线程切换，加锁解锁的时间
- redis使用io多路复用技术可以处理并发的连接

redis可以多开几个进程来避免单线程的问题，redis没有约束，只需要分辨出用户的数据是在那个服务器或者是进程上就可以

多路复用用来解决一个io阻塞影响其他io的问题

> redis 实现分布式锁

setnx命令，set if not exists 

expire 命令，设置过期时间

1. 使用lua脚本
   lua脚本保证原子性
```java
public boolean tryLock_with_lua(String key, String UniqueId, int seconds) {
    String lua_scripts = "if redis.call('setnx',KEYS[1],ARGV[1]) == 1 then" +
            "redis.call('expire',KEYS[1],ARGV[2]) return 1 else return 0 end";
    List<String> keys = new ArrayList<>();
    List<String> values = new ArrayList<>();
    keys.add(key);
    values.add(UniqueId);
    values.add(String.valueOf(seconds));
    Object result = jedis.eval(lua_scripts, keys, values);
    //判断是否成功
    return result.equals(1L);
}

```
2. 整合到一起的两个命令
```shell
SET key value[EX seconds][PX milliseconds][NX|XX]
```
```java
public boolean tryLock_with_set(String key, String UniqueId, int seconds) {
    return "OK".equals(jedis.set(key, UniqueId, "NX", "EX", seconds));
}
```

锁的释放
```java
public boolean releaseLock_with_lua(String key,String value) {
    String luaScript = "if redis.call('get',KEYS[1]) == ARGV[1] then " +
            "return redis.call('del',KEYS[1]) else return 0 end";
    return jedis.eval(luaScript, Collections.singletonList(key), Collections.singletonList(value)).equals(1L);
}

```

> redis主从，哨兵，集群有什么区别

Redis主从就是常见的主从模式，从节点自动同步主节点数据，实现数据的热备份。

Redis哨兵就是在Redis主从上添加了一个监控系统（Redis Sentinel系统），实现故障转移，Redis哨兵会监控Redis主从节点运行状态，当主节点故障下线后，Redis哨兵会选择一个从节点充当新的主节点，继续提供服务。

Redis集群在Redis主从上添加了监控机制和数据分片机制（Redis中是分槽位），实现故障转移和数据水平扩展，Redis集群中组合了多个Redis主从，并且每个Redis主节点都负责存储集群中的一部分数据，当某个主节点故障下线后，Redis集群会选择该节点的一个从节点充当新的主节点，继续提供服务。

生产环境应该很少使用单纯的Redis主从吧，如果数据量比较少，可以使用哨兵模式，但Redis集群的稳定性、可扩展性都优于哨兵模式，所以使用Redis集群的场景应该是最多的吧。
# 服务器
# 网络







### tcp 三次握手
> TCP三次握手过程

TCP(Transmission Control Protocol)　传输控制协议

tcp的6种标志位的分别代表：
- `SYN`(synchronous建立联机)
- `ACK`(acknowledgement 确认)
- `PSH`(push传送)
- `FIN`(finish结束)
- `RST`(reset重置)
- `URG`(urgent紧急)
- `Sequence number`(顺序号码)
- `Acknowledge number`(确认号码)

客户端TCP状态迁移：
- `CLOSED`->`SYN_SENT`->`ESTABLISHED`->`FIN_WAIT_1`->`FIN_WAIT_2`->`TIME_WAIT`->`CLOSED`

服务器TCP状态迁移：
- `CLOSED`->`LISTEN`->`SYN`收到->`ESTABLISHED`->`CLOSE_WAIT`->`LAST_AC`K->`CLOSED`

- `LISTEN` - 侦听来自远方TCP端口的连接请求；
- `SYN-SENT` -在发送连接请求后等待匹配的连接请求；
- `SYN-RECEIVED` - 在收到和发送一个连接请求后等待对连接请求的确认；
- `ESTABLISHED`- 代表一个打开的连接，数据可以传送给用户；
- `FIN-WAIT-1` - 等待远程TCP的连接中断请求，或先前的连接中断请求的确认；
- `FIN-WAIT-2` - 从远程TCP等待连接中断请求；
- `CLOSE-WAIT` - 等待从本地用户发来的连接中断请求；
- `CLOSING` -等待远程TCP对连接中断的确认；
- `LAST-ACK` - 等待原来发向远程TCP的连接中断请求的确认；
- `TIME-WAIT` -等待足够的时间以确保远程TCP接收到连接中断请求的确认；
- `CLOSED` - 没有任何连接状态；

（1）第一次握手：建立连接时，客户端A发送SYN包（SYN=j）到服务器B，并进入SYN_SEND状态，等待服务器B确认。

（2）第二次握手：服务器B收到SYN包，必须确认客户A的SYN（ACK=j+1），同时自己也发送一个SYN包（SYN=k），即SYN+ACK包，此时服务器B进入`SYN_RECV`状态。

（3）第三次握手：客户端A收到服务器B的SYN＋ACK包，向服务器B发送确认包ACK（ACK=k+1），此包发送完毕，客户端A和服务器B进入`ESTABLISHED`状态，完成三次握手。

完成**三次握手**，客户端与服务器开始传送数据。

确认号：其数值等于发送方的发送序号 +1(即接收方期望接收的下一个序列号)。


>TCP发数据过程中必须按顺序接收吗

有序号，是没必要按顺序接受的

>四次挥手，关闭连接

由于TCP连接是***全双工***的，因此每个方向都必须单独进行关闭。这个原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。

CP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。

（1）客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送。

（2）服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号。

（3）服务器B关闭与客户端A的连接，发送一个FIN给客户端A。

（4）客户端A发回ACK报文确认，并将确认序号设置为收到序号加1。

一个TCP连接存在双向的读写通道。

简单说来是 “先关读，后关写”，一共需要四个阶段。以客户机发起关闭连接为例：
- 1.服务器读通道关闭
- 2.客户机写通道关闭
- 3.客户机读通道关闭
- 4.服务器写通道关闭

> 三次握手的目的

为了防止A端已经失效的连接请求再次发送到B端，如果这时B端又再次向A端发出确认连接请求造成错误。

为了避免重复连接。

# 操作系统
## Volatile如何保证线程可见性之总线锁、缓存一致性协议

>Volatile如何保证线程可见性之总线锁、缓存一致性协议

### 缓存一致性协议
>多线程多cpu导致的数据不同步，存在缓存不一致性

操作系统提供了**总线锁定**的机制。前端总线(也叫CPU总线，Front Side Bus）)是所有CPU与芯片组连接的主干道，负责CPU与外界所有部件的通信，包括高速缓存、内存、北桥，其控制总线向各个部件发送控制信号、通过地址总线发送地址信号指定其要访问的部件、通过数据总线双向传输。在CPU1要做 i++操作的时候，其在总线上发出一个`LOCK`#信号，其他处理器就不能操作缓存了该共享变量内存地址的缓存，也就是阻塞了其他CPU，使该处理器可以独享此共享内存。

但我们只需要对此共享变量的操作是原子就可以了，而总线锁定把CPU和内存的通信给锁住了，使得在锁定期间，其他处理器不能操作其他内存地址的数据，从而开销较大，所以后来的CPU都提供了缓存一致性机制，Intel的奔腾486之后就提供了这种优化。

缓存一致性：缓存一致性机制就整体来说，是当某块CPU对缓存中的数据进行操作了之后，就通知其他CPU放弃储存在它们内部的缓存，或者从主内存中重新读取, 用MESI阐述原理如下：

`MESI`协议：是以缓存行(缓存的基本数据单位，在Intel的CPU上一般是64字节)的几个状态来命名的(全名是`Modified`、`Exclusive`、 `Share or Invalid`)。该协议要求在每个缓存行上维护两个状态位，使得每个数据单位可能处于M、E、S和I这四种状态之一，各种状态含义如下：

-​ M：被修改的。处于这一状态的数据，只在本CPU中有缓存数据，而其他CPU中没有。同时其状态相对于内存中的值来说，是已经被修改的，且没有更新到内存中。
- ​E：独占的。处于这一状态的数据，只有在本CPU中有缓存，且其数据没有修改，即与内存中一致。
- ​S：共享的。处于这一状态的数据在多个CPU中都有缓存，且与内存一致。
​- I：无效的。本CPU中的这份缓存已经无效。

一个处于M状态的缓存行，必须时刻监听所有试图读取该缓存行对应的主存地址的操作，如果监听到，则必须在此操作执行前把其缓存行中的数据写回内存。

一个处于S状态的缓存行，必须时刻监听使该缓存行无效或者独享该缓存行的请求，如果监听到，则必须把其缓存行状态设置为I。

一个处于E状态的缓存行，必须时刻监听其他试图读取该缓存行对应的主存地址的操作，如果监听到，则必须把其缓存行状态设置为S。

​ 当CPU需要读取数据时，如果其缓存行的状态是I的，则需要从内存中读取，并把自己状态变成S，如果不是I，则可以直接读取缓存中的值，但在此之前，必须要等待其他CPU的监听结果，如其他CPU也有该数据的缓存且状态是M，则需要等待其把缓存更新到内存之后，再读取。

​ 当CPU需要写数据时，只有在其缓存行是M或者E的时候才能执行，否则需要发出特殊的RFO指令(Read Or Ownership，这是一种总线事务)，通知其他CPU置缓存无效(I)，这种情况下性能开销是相对较大的。在写入完成后，修改其缓存状态为M。

所以如果一个变量在某段时间只被一个线程频繁地修改，则使用其内部缓存就完全可以办到，不涉及到总线事务，如果缓存一会被这个CPU独占、一会被那个CPU 独占，这时才会不断产生RFO指令影响到并发性能。这里说的缓存频繁被独占并不是指线程越多越容易触发，而是这里的CPU协调机制，这有点类似于有时多线程并不一定提高效率，原因是线程挂起、调度的开销比执行任务的开销还要大，这里的多CPU也是一样，如果在CPU间调度不合理，也会形成RFO指令的开销比任务开销还要大。当然，这不是编程者需要考虑的事，操作系统会有相应的内存地址的相关判断
#### MESI失效的情景
>MESI失效的情景

并非所有情况都会使用缓存一致性，如被操作的数据不能被缓存在CPU内部或操作数据跨越多个缓存行(状态无法标识)，则处理器会调用总线锁定;另外当CPU不支持缓存锁定时，自然也只能用总线锁定了，比如说奔腾486以及更老的CPU。总线事务的竞争，虽然有很高的一致性但是效率非常低。

缓存一致性协议有多种，但是日常处理的大多数计算机设备都属于”嗅探（snooping）”机制，它的基本思想是：
所有内存的传输都发生在一条共享的总线上，而所有的处理器都能看到这条总线：缓存本身是独立的，但是内存是共享资源，所有的内存访问都要经过仲裁（同一个指令周期中，只有一个CPU缓存可以读写内存）。
CPU缓存不仅仅在做内存传输的时候才与总线打交道，而是不停在嗅探总线上发生的数据交换，跟踪其他缓存在做什么。所以当一个缓存代表它所属的处理器去读写内存时，其它处理器都会得到通知，它们以此来使自己的缓存保持同步。只要某个处理器一写内存，其它处理器马上知道这块内存在它们的缓存段中已失效。

#### MESI协议的问题
>既然CPU有了MESI协议可以保证cache的一致性，那么为什么还需要volatile这个关键词来保证可见性(内存屏障)？或者是只有加了volatile的变量在多核cpu执行的时候才会触发缓存一致性协议？

两个解释结论：
<!-- 因为volatile关键字保证可见性. -->

多核情况下，所有的cpu操作都会涉及缓存一致性的校验，只不过该协议是弱一致性，不能保证一个线程修改变量后，其他线程立马可见，也就是说虽然其他CPU状态已经置为无效，但是当前CPU可能将数据修改之后又去做其他事情，没有来得及将修改后的变量刷新回主存，而如果此时其他CPU需要使用该变量，则又会从主存中读取到旧的值。而volatile则可以保证可见性，即立即刷新回主存，修改操作和写回操作必须是一个原子操作；
正常情况下，系统操作并不会进行缓存一致性的校验，只有变量被volatile修饰了，该变量所在的缓存行才被赋予缓存一致性的校验功能。
## ReentrantLock和Synchronized区别，公平锁和非公平锁区别

> `ReentrantLock`和`Synchronized`区别，公平锁和非公平锁区别

- (1) `synchronized` 是Java的一个内置关键字，而`ReentrantLock`是Java的一个类。
- (2) `synchronized`只能是非公平锁。而`ReentrantLock`可以实现公平锁和非公平锁两种。
- (3) `synchronized`不能中断一个等待锁的线程，而`Lock`可以中断一个试图获取锁的线程。
- (4) `synchronized`不能设置超时，而`Lock`可以设置超时。
- (5) `synchronized`会自动释放锁，而`ReentrantLock`不会自动释放锁，必须手动释放，否则可能会导致死锁。

```java
/**
	 * 公平锁实现 ReentrantLock构造方法中设置为true:代表公平锁
	 * 
	 * 设置为false:代表非公平锁 默认也是非公平锁
	 * 
	 */
	/** private ReentrantLock lock = new ReentrantLock(true); */
 
	/** private ReentrantLock lock = new ReentrantLock(false);
```

>公平锁和非公平锁的两个区别

- (1) 线程在获取锁调用lock()时，非公平锁首先会进行一次CAS尝试抢锁，如果此时没有线程持有锁或者正好此刻有线程执行完释放了锁（`state` == 0），那么如果CAS成功则直接占用锁返回。
- (2) 如果非公平锁在上一步获取锁失败了，那么就会进入`nonfairTryAcquire(int acquires)`，在该方法里，如果state的值为0，表示当前没有线程占用锁或者刚好有线程释放了锁，那么就会CAS抢锁，如果抢成功了，就直接返回了，不管是不是有其他线程早就到了在阻塞队列中等待锁了。而公平锁在这里抢到锁了，会判断阻塞队列是不是空的，毕竟要公平就要讲先来后到，如果发现阻塞队列不为空，表示队列中早有其他线程在等待了，那么公平锁情况下线程会乖乖排到阻塞队列的末尾。
  如果非公平锁 (1)(2) 都失败了，那么剩下的过程就和非公平锁一样了。
- (3) 从(1)(2) 可以看出，非公平锁可能导致线程饥饿，但是非公平锁的效率要高

> 常见的锁策略

1、乐观锁和悲观锁

乐观锁和悲观锁是设计上解决线程安全一种思想

悲观锁: 悲观的认为总是有其他线程并发修改，每次都是加锁操作

悲观锁的问题：总是需要竞争锁，进而导致发生线程切换，挂起其他线程；所以性能不高。

乐观锁: 设计上总是乐观的认为数据修改大部分场景都是没有线程并发修改，只有少量情况下才存在。线程安全上采取版本号来控制——用户自己判断版本号，并处理。（版本号后面讲）

乐观锁的问题： 并不总是能处理所有问题，所以会引入一定的系统复杂度。

2、读写锁（`readers-writer lock`）

问题产生： 多线程之间，数据的读取方之间不会产生线程安全问题，但数据的写入方互相之间以及和读者之间都需要进行互斥。如果两种场景下都用同一个锁，就会产生极大的性能损耗。所以读写锁因此而产生。

读写锁（`readers-writer lock`）： 看英文可以顾名思义，在执行加锁操作时需要额外表明读写意图，读者之间并不互斥，而写者则要求与任何人互斥。

3、重量级锁和轻量级锁

我们都知道，在计算机中，应用程序使用计算机的资源都是通过程序调用操作系统再到硬件这一步一步来的 

重量级锁：
操作系统中提供的`mutex `（互斥锁）就是一个重量级锁，这种锁同步方式的成本非常高，主要包括系统调用引起的内核态与用户态切换、线程阻塞造成的线程切换等。

缺点： 成本高，很容易引起线程大量调度。

轻量级锁：
重量级锁是相当于重量级锁而言的，将重量级锁在操作系统的内核态操作转为大量的用户态操作。
优点： 减少了无实际竞争情况下，使用重量级锁产生的性能消耗

4、自旋锁（Spin Lock）

没有自旋锁的情况下，抢锁失败，就赶紧放弃CPU，到阻塞队列中等待重新被唤醒，重新抢CPU，重新抢锁。

但是CPU来之不易，放弃CPU就可能需要过很久才能再次被调度。
随着多核 CPU的出现，情况发生变化了;这个锁的线程，很可能正在其他核上运行着,一个锁的持有时间一般都比较短,虽然现在没抢到锁，但很可能在几个时钟周期之后，就能马上抢到锁了。因此，线程就可以不用放弃cpu,多等待一会，就有可能抢到锁。出于这个考虑，就产生了自旋锁。

我们可以将自旋锁简单的理解为以下代码：

自旋锁的缺点： 如果之前的假设（锁很快会被释放）没有满足，则线程其实是光在消耗 CPU 资源，长期在做无用功的。

5、可重入锁和不可重入锁

可重入锁的字面意思是“可以重新进入的锁”，即允许同一个线程多次获取同一把锁。 比如一个递归函数里有加锁操作，递归过程中这个锁会阻塞自己吗？如果不会，那么这个锁就是可重入锁（因为这个原因可重入锁也叫做递归锁）。

Java里只要以`Reentrant`开头命名的锁都是可重入锁，而且JDK提供的所有现成的Lock实现类，包括synchronized关键字锁都是可重入的。

可重入锁的实现是有成本的：

空间上:需要记录锁的持有者
时间上:判断，并记录次数（请求释放数量需要对应上）
管理是有成本的，可重入锁看起来很理想，但需要考虑是否有必要引入该成本。

不可重入锁相对于可重入锁而言，不能重新获取的锁为不可重入锁，也就是不允许同一个线程多次获得同一把锁。

> cas 机制

CAS机制
1、CAS
CAS: 全称Compare and swap，字面意思:“比较并交换

为了提升synchronized保证原子性的效率问题，我们引出了CAS机制

CAS是CPU(硬件） 在电路上来提供提供保证原子性的操作，进而提供给OS（操作系统）、JVM，操作系统和JVM也就可以提供CAS来保证原子性。

jdk5增加的原子性并发包( `java.util.concurrent.*`)，里边的所有并发类都是基于CAS实现的,是一种乐观锁。 JDK 5之前Java语言是靠synchronized关键字保证同步的，这是一种独占锁，也是是悲观锁.

当多个线程同时对某个资源进行CAS操作，只能有一个线程操作成功，但是并不会阻塞其他线程,其他线程只会收到操作失败的信号。可见 CAS 是基于乐观锁实现的

>死锁

多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。 由于线程被无限期地阻塞，因此程序不可能正常终止，就会造成死锁。

即：至少有两个线程，互相持有对方申请的对象锁，造成互相等待。导致没法继续执行
死锁产生的四个必要条件：

- 互斥使用： 即当资源被一个线程使用(占有)时，别的线程不能使用
- 不可抢占： 资源请求者不能强制从资源占有者手中夺取资源，资源只能由资源占有者主动释放。
- 请求和保持： 即当资源请求者在请求其他的资源的同时保持对原有资源的占有。
- 循环等待： 即存在一个等待队列：P1占有P2的资源，P2占有P3的资源，P3占有P1的资源。这样就形成了一个等待环路。

当上述四个条件都成立的时候，便形成死锁。当然，死锁的情况下如果打破上述任何一个条件，便可让死锁消失。

检测死锁的手段:

使用jdk的监控工具，比如`jconsole`、`jstack`查看线程状态

解决方案:

①资源—次性分配（破坏请求与保持条件)
②可剥夺资源:在线程满足条件时，释放掉已占有的资源
③资源有序分配:系统为每类资源赋予一个编号，每个线程按照编号递请求资源，释放则相反
超时释放锁
等待图主动检测死锁 wait-for-graph，保存锁的信息链表，事务等待链表。
# linux
# 算法


# 其他知识


框架-》模板-》背景

测试用例，自己写测试考虑测试用例，考虑代码质量，覆盖率   通过测试用例提升自己的代码质量

代码上线，部署到分布式服务器或者云上服务器 需要有一个前端展示界面

做压力测试，比如能够承载多少访问量


难点 
注入fgc的bug

并发的问题